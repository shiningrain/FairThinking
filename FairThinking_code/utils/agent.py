import openai
import backoff
import time
import random
from openai.error import RateLimitError, APIError, ServiceUnavailableError, APIConnectionError
import sys
from openai_utils import num_tokens_from_string
import time
from inferenceLLM import load_llama_model,inference_llama_model,inference_vicuna_model,inference_mistral_model
from llama_recipes.inference.chat_utils import format_tokens
import copy

support_models = ['gpt-3.5-turbo', 'gpt-4-1106-preview']

class Agent:
    def __init__(self, model_name: str, name: str, temperature: float, sleep_time: float=0) -> None:
        """Create an agent

        Args:
            model_name(str): model name
            name (str): name of this agent
            temperature (float): higher values make the output more random, while lower values make it more focused and deterministic
            sleep_time (float): sleep because of rate limits
        """
        self.model_name = model_name
        self.name = name
        self.temperature = temperature
        self.memory_lst = []
        self.sleep_time = sleep_time

    # @backoff.on_exception(backoff.expo, (RateLimitError, APIError, ServiceUnavailableError, APIConnectionError), max_tries=20)
    def query(self, messages: "list[dict]", max_tokens: int, api_key: str, temperature: float) -> str:
        """make a query

        Args:
            messages (list[dict]): chat history in turbo format
            max_tokens (int): max token in api call
            api_key (str): openai api key
            temperature (float): sampling temperature

        Raises:
            OutOfQuotaException: the apikey has out of quota
            AccessTerminatedException: the apikey has been ban

        Returns:
            str: the return msg
        """
        time.sleep(self.sleep_time)
        assert self.model_name in support_models, f"Not support {self.model_name}. Choices: {support_models}"
        try:
            if self.model_name in support_models:
                response = openai.ChatCompletion.create(
                    model=self.model_name,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    api_key=api_key,
                    request_timeout=90,#timeout=90, in 1.x
                )
                gen = response['choices'][0]['message']['content']
            return gen
        except Exception as e:
            time.sleep(60)
            if self.model_name in support_models:
                response = openai.ChatCompletion.create(
                    model=self.model_name,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    api_key=api_key,
                    request_timeout=90,#timeout=90, in 1.x
                )
                gen = response['choices'][0]['message']['content']
            return gen

    def set_meta_prompt(self, meta_prompt: str):
        """Set the meta_prompt

        Args:
            meta_prompt (str): the meta prompt
        """
        self.memory_lst.append({"role": "system", "content": f"{meta_prompt}"})

    def add_event(self, event: str):
        """Add an new event in the memory

        Args:
            event (str): string that describe the event.
        """
        self.memory_lst.append({"role": "user", "content": f"{event}"})

    def add_memory(self, memory: str):
        """Monologue in the memory

        Args:
            memory (str): string that generated by the model in the last round.
        """
        self.memory_lst.append({"role": "assistant", "content": f"{memory}"})
        print(f"----- {self.name} -----\n{memory}\n")

    def ask(self, max_token=None, temperature=None):
        """Query for answer

        Args:
        """
        # query
        if self.model_name=='gpt-3.5-turbo' or self.model_name=='gpt-4-1106-preview':
            time.sleep(3)
            num_context_token = sum([num_tokens_from_string(m["content"], self.model_name) for m in self.memory_lst])
            if len(self.memory_lst)>3:
                query_memory=precess_memory_list(self.memory_lst) # make sure system prompt is new than assistant anwser and reduce the failed role-playing
            else:
                query_memory=self.memory_lst
            result=self.query(query_memory, max_token, api_key=self.openai_api_key, temperature=temperature if temperature else self.temperature)
            if self.model_name=='gpt-4-1106-preview' and '```json' in result:
                try:
                    result=result[result.index('{'):result.index('}')+1]
                except:
                    print('Error GPT4 json:\n'+result)
                    result=result
            return result
        elif self.model_name=='text-curie-001' or self.model_name=='davinci-002': # need openai<=0.28
            
            time.sleep(3)
            prompt=get_prompt(self.memory_lst)
            result=''
            count=0
            while result=='':
                completion = openai.Completion.create(
                    model=self.model_name,
                    prompt=prompt,
                    api_key=self.openai_api_key,
                    max_tokens=max_token,
                    n=1)
                result=completion['choices'][0]._previous['text']
                if result=='':
                    print(count)
                count+=1
                time.sleep(5)
                if count>10:
                    break
            return result
        elif self.model_name=='llama':
            max_token=max_token+500 # they will generate original input
            if len(self.memory_lst)>3:
                query_memory=precess_memory_list(self.memory_lst) # make sure system prompt is new than assistant anwser and reduce the failed role-playing
            else:
                query_memory=self.memory_lst
            prompt=format_tokens([query_memory],self.tokenizer)[0]
            result=inference_llama_model(self.model,self.tokenizer,prompt,max_new_tokens=max_token)
            result = result.split('[/INST] ')[-1]
            return result
        elif self.model_name=='vicuna':
            max_token=max_token+500 # they will generate original input
            if len(self.memory_lst)>3:
                query_memory=precess_memory_list(self.memory_lst) # make sure system prompt is new than assistant anwser and reduce the failed role-playing
            else:
                query_memory=self.memory_lst
            result=generate_vicuna_answer(self.model,self.tokenizer,query_memory,max_token=max_token)
            return result
        elif self.model_name=='mistral':
            max_token=max_token+500 # they will generate original input
            if len(self.memory_lst)>3:
                query_memory=precess_memory_list(self.memory_lst) # make sure system prompt is new than assistant anwser and reduce the failed role-playing
            else:
                query_memory=self.memory_lst
            result=generate_mistral_answer(self.model,self.tokenizer,query_memory,max_token=max_token)
            return result

def precess_memory_list(memory_lst):
    query_memory_list=copy.deepcopy(memory_lst)
    last_index= max((index, d) for index, d in enumerate(query_memory_list) if d['role'] == 'assistant')[0]
    query_memory_list.insert(last_index + 1, query_memory_list[0])
    # make LLM not foget its role
    return query_memory_list[last_index+1:]#query_memory_list[1:] #

def get_prompt(context_list):
    if len(context_list)<=3:
        tmp_list=[context_list[i]['content'] for i in range(len(context_list))]
    else:
        tmp_list=[context_list[i]['content'] for i in range(len(context_list)) if context_list[i]['role']=='user'][-2:]
    context_string='///'.join(tmp_list)
    return context_string

def get_vicuna_input(answer_context):
    input_text='\n'.join(['[{}]: {}\n'.format(i['role'],i['content']) for i in answer_context])+'\n[assistant]:'
    return input_text

def generate_vicuna_answer(model,tokenizer,answer_context,max_token=200):
    # input_prompt=format_tokens([answer_context],tokenizer)[0]
    # result = inference_llama_model(model=model,tokenizer=tokenizer,user_prompt=input_prompt,max_new_tokens=max_token)
    input_text=get_vicuna_input(answer_context)
    # input_ids = tokenizer(input_text, return_tensors="pt").to('cuda')
    # out = model.generate(input_ids['input_ids'], max_new_tokens=max_token,do_sample=True)
    # result = tokenizer.decode(out[0]).split('\nYour Answer:')[-1].replace('</s>','')
    result=inference_vicuna_model(model,tokenizer,input_text,max_new_tokens=max_token)
    return result

def process_mistral_answer(answer_context):
    new_answer_context=[]
    for context in answer_context:
        if context['role']=='system':
            tmp=copy.deepcopy(context)
            tmp['role']='user'
            new_answer_context.append(tmp)
            new_answer_context.append({'role':'assistant','content': 'Sure, no problem!'})
        else:
            new_answer_context.append(context)
    return new_answer_context

def generate_mistral_answer(model,tokenizer,answer_context,max_token=200):
    # print('========using mistral inference API!!========')
    # new_answer_context=process_mistral_answer(answer_context)# solve the known Jinji bug: Conversation roles must alternate user/assistant/user/assistant/...
    # result=inference_mistral_model(model,tokenizer,new_answer_context,max_new_tokens=max_token)
    result=generate_vicuna_answer(model,tokenizer,answer_context,max_token=max_token)
    return result